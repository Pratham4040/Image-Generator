{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pratham4040/Image-Generator/blob/main/GPT_SoVITS_WebUI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-SoVITS-WebUI\n",
        "A Few-shot & Zero-shot Text-to-Speech WebUI.\n",
        "\n",
        "\n",
        "[General GPT-SoVITS Guide](https://rentry.co/GPT-SoVITS-guide#3-initialize-dataset)\n",
        "\n",
        "### Credits:\n",
        "\n",
        "- Original Project & Colab done by [GPT-SoVITS Team](https://github.com/RVC-Boss/GPT-SoVITS/graphs/contributors)\n",
        "\n",
        "- Port made by [Nick088](https://linktr.ee/Nick088)\n",
        "\n",
        "- Testing/Tweaks done by [vulkanitexd](https://discord.com/users/984567398826917918)"
      ],
      "metadata": {
        "id": "_oJOFXTQ0OhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Prepare the Environment\n",
        "#@markdown It git clones the repository.\n",
        "\n",
        "# get the repo\n",
        "!git clone https://github.com/RVC-Boss/GPT-SoVITS\n",
        "%cd GPT-SoVITS"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RvOEAQx-9wBP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebb712b5-1203-4e64-d311-4971e32d084c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GPT-SoVITS'...\n",
            "remote: Enumerating objects: 4289, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 4289 (delta 36), reused 17 (delta 17), pack-reused 4244 (from 4)\u001b[K\n",
            "Receiving objects: 100% (4289/4289), 12.63 MiB | 5.49 MiB/s, done.\n",
            "Resolving deltas: 100% (2477/2477), done.\n",
            "/content/GPT-SoVITS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Conda\n",
        "#@markdown Conda will make the session crash/restart, it's normal, just run the following cells after.\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install_from_url(\"https://repo.anaconda.com/miniconda/Miniconda3-py39_24.11.1-0-Linux-x86_64.sh\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fMsdGH8AlQgn",
        "outputId": "ba10e411-8131-4e37-9c4d-d2a8eeb55836",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬ Downloading https://repo.anaconda.com/miniconda/Miniconda3-py39_24.11.1-0-Linux-x86_64.sh...\n",
            "ðŸ“¦ Installing...\n",
            "ðŸ“Œ Adjusting configuration...\n",
            "ðŸ©¹ Patching environment...\n",
            "â² Done in 0:00:26\n",
            "ðŸ” Restarting kernel...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install requirements\n",
        "#@markdown Will take alot of time..\n",
        "\n",
        "#@markdown The runtime will restart itself, it's normal.\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "\n",
        "%cd /content/GPT-SoVITS/\n",
        "!bash install.sh\n",
        "\n",
        "# fix packages asr models japanese/english (whisper)\n",
        "!pip install ctranslate2==4.4.0\n",
        "\n",
        "# fix https://github.com/RVC-Boss/GPT-SoVITS/issues/1715\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# fix for issue in inference\n",
        "!pip install -q ipykernel\n",
        "\n",
        "# fix train gpt\n",
        "!pip install torchmetrics==1.5\n",
        "\n",
        "clear_output()\n",
        "\n",
        "print(\"Installed the requirements!\")\n",
        "\n",
        "time.sleep(5)\n",
        "# restart runtime\n",
        "import os\n",
        "os._exit(0)"
      ],
      "metadata": {
        "id": "afNtr2lXyuKQ",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1be0fa4f-7af9-4988-b3c4-4f6e2455a763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installed the requirements!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Pretrained Models\n",
        "#@markdown gpt so vits v1 & v2 pretrains + uvr5 weights + chinese asr. (english/japanese asr, faster whisper, should be automatically installed when using it for ASR in the UI)\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "\n",
        "# gpt so vits pretrains\n",
        "\n",
        "!git clone https://huggingface.co/lj1995/GPT-SoVITS/ gptsovits_pretrains_temp\n",
        "\n",
        "!mv gptsovits_pretrains_temp/* GPT-SoVITS/GPT_SoVITS/pretrained_models/\n",
        "\n",
        "!rm -rf gptsovits_pretrains_temp\n",
        "\n",
        "\n",
        "# uvr5 weights\n",
        "\n",
        "!git clone https://huggingface.co/Delik/uvr5_weights uvr5_weights_temp\n",
        "\n",
        "!mv uvr5_weights_temp/* GPT-SoVITS/tools/uvr5/uvr5_weights\n",
        "\n",
        "!rm -rf uvr5_weights_temp\n",
        "\n",
        "\n",
        "# Chinese ASR models\n",
        "\n",
        "%cd /content/GPT-SoVITS/tools/asr/models\n",
        "!git clone https://www.modelscope.cn/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch.git\n",
        "!git clone https://www.modelscope.cn/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch.git\n",
        "!git clone https://www.modelscope.cn/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch.git\n",
        "\n",
        "# English/Japanese ASR should be automatically downloaded.\n",
        "\n",
        "clear_output()\n",
        "print(\"Downloaded!\")"
      ],
      "metadata": {
        "id": "ym6l8aAiBjKj",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be9a551f-4518-4d6d-fc28-c031ff4ddd98"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run UI\n",
        "\n",
        "#@markdown The first run may take more time as it will download the g2pw model.\n",
        "\n",
        "#@markdown The type of tunnel you wanna use for seeing the public link, so that if one of them is down, you can use the other one.\n",
        "Tunnel = \"Gradio\" #@param [\"Gradio\", \"Ngrok\", \"Cloudflare\", \"LocalTunnel\", \"Horizon\"]\n",
        "\n",
        "#@markdown Also when using Ngrok ,Cloudflare, LocalTunnel, or Horizon, you need to wait for the Local URL to appear, and only after that click on the Public URL which is above. Plus, you have to open UVR in the GPT-SoVITS UI before opening the Public URL.\n",
        "\n",
        "\n",
        "#@markdown Use the following option **only if you chose Ngrok** as the Tunnel, **You need to be a paid ngrok member** as it needs more than 3 tunnels (there are 4 ports):\n",
        "\n",
        "#@markdown You can get the Ngrok Tunnel Authtoken here: https://dashboard.ngrok.com/tunnels/authtokens/new.\n",
        "\n",
        "ngrok_tunnel_authtoken = \"\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "#@markdown Use the following option **only if you chose Horizon** as the Tunnel:\n",
        "\n",
        "#@markdown You can get the Horizon ID here: https://hrzn.run/dashboard/ , login, on the 2nd step, it shows an `hrzn login YOUR_ID`, you need to copy that id.\n",
        "\n",
        "horizon_id = \"\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "\n",
        "%cd /content/GPT-SoVITS/\n",
        "\n",
        "\n",
        "if Tunnel == \"Gradio\":\n",
        "  %env is_share = True\n",
        "elif Tunnel == \"Ngrok\":\n",
        "  %env is_share = False\n",
        "  !pip install pyngrok\n",
        "  from pyngrok import ngrok\n",
        "  ngrok.set_auth_token(ngrok_tunnel_authtoken)\n",
        "  gptsovits_tunnel = ngrok.connect(9874, bind_tls=True)\n",
        "  uvr_tunnel = ngrok.connect(9873, bind_tls=True)\n",
        "  inference_tunnel = ngrok.connect(9872, bind_tls=True)\n",
        "  proofreading_tunnel = ngrok.connect(9871, bind_tls=True)\n",
        "  print(\"GPT-SoVITS Tunnel Public URL:\", gptsovits_tunnel.public_url)\n",
        "  print(\"UVR Tunnel Public URL:\", uvr_tunnel.public_url)\n",
        "  print(\"Inference Tunnel Public URL:\", uvr_tunnel.public_url)\n",
        "  print(\"Proofreading Tunnel Public URL:\", proofreading_tunnel.public_url)\n",
        "elif Tunnel == \"Cloudflare\":\n",
        "  %env is_share = False\n",
        "  # download cloudflare\n",
        "  !curl -LO https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "  !dpkg -i cloudflared-linux-amd64.deb\n",
        "  !rm -rf nohup.out\n",
        "  import time\n",
        "  # Run cloudflare\n",
        "  # gpt so vits\n",
        "  !nohup cloudflared tunnel --url localhost:9874 &\n",
        "  time.sleep(7)\n",
        "  # uvr\n",
        "  !nohup cloudflared tunnel --url localhost:9873 &\n",
        "  time.sleep(7)\n",
        "  # inference\n",
        "  !nohup cloudflared tunnel --url localhost:9872 &\n",
        "  time.sleep(7)\n",
        "  # proofreading\n",
        "  !nohup cloudflared tunnel --url localhost:9871 &\n",
        "  clear_output()\n",
        "  time.sleep(7)\n",
        "  # Find and print the Cloudflare URL with a prefix\n",
        "  cloudflare_url = !grep -oE \"https://[a-zA-Z0-9.-]+\\.trycloudflare\\.com\" nohup.out\n",
        "  print(f\"GPT-SoVITS Tunnel Public URL: {cloudflare_url[0]}\")\n",
        "  print(f\"UVR Tunnel Public URL: {cloudflare_url[1]}\")\n",
        "  print(f\"Inference Tunnel Public URL: {cloudflare_url[2]}\")\n",
        "  print(f\"Proofreading Tunnel Public URL: {cloudflare_url[3]}\")\n",
        "elif Tunnel == \"LocalTunnel\":\n",
        "  %env is_share = False\n",
        "  # install\n",
        "  !npm install -g localtunnel\n",
        "  import time\n",
        "  import urllib\n",
        "  # run localtunnel\n",
        "  # gptsovits\n",
        "  with open('url.txt', 'w') as file:\n",
        "        file.write('')\n",
        "\n",
        "  get_ipython().system_raw('lt --port 9874 >> url.txt 2>&1 &')\n",
        "\n",
        "  time.sleep(4)\n",
        "\n",
        "  endpoint_ip = urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\")\n",
        "\n",
        "  with open('url.txt', 'r') as file:\n",
        "      tunnel_url = file.read()\n",
        "      tunnel_url = tunnel_url.replace(\"your url is: \", \"\")\n",
        "\n",
        "  clear_output()\n",
        "\n",
        "  print(f\"GPT-SoVITS Tunnel Public URL: \\033[0m\\033[93m{tunnel_url}\\033[0m\", end=\"\\033[0m\")\n",
        "\n",
        "  # uvr\n",
        "  with open('url2.txt', 'w') as file:\n",
        "        file.write('')\n",
        "\n",
        "  get_ipython().system_raw('lt --port 9873 >> url2.txt 2>&1 &')\n",
        "\n",
        "  time.sleep(4)\n",
        "\n",
        "  endpoint_ip = urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\")\n",
        "\n",
        "  with open('url2.txt', 'r') as file:\n",
        "      tunnel_url = file.read()\n",
        "      tunnel_url = tunnel_url.replace(\"your url is: \", \"\")\n",
        "\n",
        "  print(f\"UVR Tunnel Public URL: \\033[0m\\033[93m{tunnel_url}\\033[0m\", end=\"\\033[0m\")\n",
        "\n",
        "  # inference\n",
        "  with open('url3.txt', 'w') as file:\n",
        "        file.write('')\n",
        "\n",
        "  get_ipython().system_raw('lt --port 9872 >> url3.txt 2>&1 &')\n",
        "\n",
        "  time.sleep(4)\n",
        "\n",
        "  endpoint_ip = urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\")\n",
        "\n",
        "  with open('url3.txt', 'r') as file:\n",
        "      tunnel_url = file.read()\n",
        "      tunnel_url = tunnel_url.replace(\"your url is: \", \"\")\n",
        "\n",
        "  print(f\"Inference Tunnel Public URL: \\033[0m\\033[93m{tunnel_url}\\033[0m\", end=\"\\033[0m\")\n",
        "\n",
        "  # proofreading\n",
        "  with open('url4.txt', 'w') as file:\n",
        "        file.write('')\n",
        "\n",
        "  get_ipython().system_raw('lt --port 9871 >> url4.txt 2>&1 &')\n",
        "\n",
        "  time.sleep(4)\n",
        "\n",
        "  endpoint_ip = urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\")\n",
        "\n",
        "  with open('url4.txt', 'r') as file:\n",
        "      tunnel_url = file.read()\n",
        "      tunnel_url = tunnel_url.replace(\"your url is: \", \"\")\n",
        "\n",
        "  print(f\"Proofreading Tunnel Public URL: \\033[0m\\033[93m{tunnel_url}\\033[0m\", end=\"\\033[0m\")\n",
        "\n",
        "\n",
        "  print(f'LocalTunnels Password: {endpoint_ip}')\n",
        "elif Tunnel == \"Horizon\":\n",
        "  # install\n",
        "  !npm install -g @hrzn/cli\n",
        "  # login\n",
        "  !hrzn login $horizon_id\n",
        "  # run horizon\n",
        "  # gptsovits\n",
        "  with open('url.txt', 'w') as file:\n",
        "        file.write('')\n",
        "\n",
        "  get_ipython().system_raw('hrzn tunnel http://localhost:9874 >> url.txt 2>&1 &')\n",
        "\n",
        "  time.sleep(4)\n",
        "\n",
        "  with open('url.txt', 'r') as file:\n",
        "      tunnel_url = file.read()\n",
        "      tunnel_url = !grep -oE \"https://[a-zA-Z0-9.-]+\\.hrzn\\.run\" url.txt\n",
        "      tunnel_url = tunnel_url[0]\n",
        "\n",
        "  clear_output()\n",
        "\n",
        "  print(f\"GPT-SoVITS Tunnel Public URL: \\033[0m\\033[93m{tunnel_url}\\033[0m\")\n",
        "\n",
        "  # uvr\n",
        "  with open('url.txt', 'w') as file:\n",
        "        file.write('')\n",
        "\n",
        "  get_ipython().system_raw('hrzn tunnel http://localhost:9873 >> url2.txt 2>&1 &')\n",
        "\n",
        "  time.sleep(4)\n",
        "\n",
        "  with open('url2.txt', 'r') as file:\n",
        "      tunnel_url = file.read()\n",
        "      tunnel_url = !grep -oE \"https://[a-zA-Z0-9.-]+\\.hrzn\\.run\" url2.txt\n",
        "      tunnel_url = tunnel_url[0]\n",
        "\n",
        "  print(f\"UVR Tunnel Public URL: \\033[0m\\033[93m{tunnel_url}\\033[0m\")\n",
        "\n",
        "  # inference\n",
        "  with open('url3.txt', 'w') as file:\n",
        "        file.write('')\n",
        "\n",
        "  get_ipython().system_raw('hrzn tunnel http://localhost:9872 >> url3.txt 2>&1 &')\n",
        "\n",
        "  time.sleep(4)\n",
        "\n",
        "  with open('url3.txt', 'r') as file:\n",
        "      tunnel_url = file.read()\n",
        "      tunnel_url = !grep -oE \"https://[a-zA-Z0-9.-]+\\.hrzn\\.run\" url3.txt\n",
        "      tunnel_url = tunnel_url[0]\n",
        "\n",
        "  print(f\"Inference Tunnel Public URL: \\033[0m\\033[93m{tunnel_url}\\033[0m\")\n",
        "\n",
        "  # proofreading\n",
        "  with open('url4.txt', 'w') as file:\n",
        "        file.write('')\n",
        "\n",
        "  get_ipython().system_raw('hrzn tunnel http://localhost:9871 >> url4.txt 2>&1 &')\n",
        "\n",
        "  time.sleep(4)\n",
        "\n",
        "  with open('url4.txt', 'r') as file:\n",
        "      tunnel_url = file.read()\n",
        "      tunnel_url = !grep -oE \"https://[a-zA-Z0-9.-]+\\.hrzn\\.run\" url4.txt\n",
        "      tunnel_url = tunnel_url[0]\n",
        "\n",
        "  print(f\"Proofreading Tunnel Public URL: \\033[0m\\033[93m{tunnel_url}\\033[0m\")\n",
        "\n",
        "\n",
        "import subprocess\n",
        "!python webui.py"
      ],
      "metadata": {
        "id": "Otq5d5-Z7xZL",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae137538-515a-4e88-995b-e271b8840bb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT-SoVITS\n",
            "env: is_share=True\n",
            "/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "Running on local URL:  http://0.0.0.0:9874\n",
            "Running on public URL: https://17e668df9f94acedff.gradio.live\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/inference_webui.py \"Auto\"\n",
            "loading sovits_v1 <All keys matched successfully>\n",
            "Running on local URL:  http://0.0.0.0:9872\n",
            "Running on public URL: https://522ea67f1b0be12fa3.gradio.live\n",
            "Actual Input Reference Text: Repeat after me , 3 ,2 ,1,1,2,3ã€‚\n",
            "Actual Input Target Text: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/queueing.py\", line 522, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1267, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 574, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 567, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2461, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 962, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 550, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 733, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui.py\", line 550, in get_tts_wav\n",
            "    text = cut1(text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui.py\", line 695, in cut1\n",
            "    inps = split(inp)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui.py\", line 676, in split\n",
            "    if todo_text[-1] not in splits:\n",
            "IndexError: string index out of range\n",
            "Actual Input Reference Text: Repeat after me , 3 ,2 ,1,1,2,3ã€‚\n",
            "Actual Input Target Text: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/queueing.py\", line 522, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1267, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 574, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 567, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2461, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 962, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 550, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 733, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui.py\", line 550, in get_tts_wav\n",
            "    text = cut1(text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui.py\", line 695, in cut1\n",
            "    inps = split(inp)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui.py\", line 676, in split\n",
            "    if todo_text[-1] not in splits:\n",
            "IndexError: string index out of range\n",
            "Actual Input Reference Text: Repeat after me , 3 ,2 ,1,1,2,3ã€‚\n",
            "Actual Input Target Text: Tajes is so hot\n",
            "Actual Input Target Text (after sentence segmentation): Tajes is so hot\n",
            "[\u001b[36m2025-02-14 17:45:07,829\u001b[0m][\u001b[32mINFO\u001b[0m] - Downloading https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin to lid.176.bin (125.2M)\u001b[0m\n",
            "INFO:robust_downloader.downloader:Downloading https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin to lid.176.bin (125.2M)\n",
            "100% 125M/125M [00:02<00:00, 49.9MB/s]\n",
            "['REPEATAFTERME,', 'ä¸‰,äºŒ,ä¸€,ä¸€,äºŒ,ä¸‰.']\n",
            "['en', 'zh']\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n",
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba_fast:Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /content/GPT-SoVITS/TEMP/jieba.cache\n",
            "DEBUG:jieba_fast:Dumping model to file cache /content/GPT-SoVITS/TEMP/jieba.cache\n",
            "Loading model cost 0.972 seconds.\n",
            "DEBUG:jieba_fast:Loading model cost 0.972 seconds.\n",
            "Prefix dict has been built succesfully.\n",
            "DEBUG:jieba_fast:Prefix dict has been built succesfully.\n",
            "Actual Input Target Text (per sentence): Tajes is so hot.\n",
            "Processed text from the frontend (per sentence): Tajes is so hot.\n",
            "  5% 79/1500 [00:04<01:15, 18.75it/s]T2S Decoding EOS [145 -> 226]\n",
            "  5% 80/1500 [00:04<01:27, 16.31it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "1.797\t12.837\t4.914\t4.575\n",
            "Actual Input Reference Text: Repeat after me , 3 ,2 ,1,1,2,3ã€‚\n",
            "Actual Input Target Text: Tom is so hot\n",
            "Actual Input Target Text (after sentence segmentation): Tom is so hot\n",
            "['REPEATAFTERME,', 'ä¸‰,äºŒ,ä¸€,ä¸€,äºŒ,ä¸‰.']\n",
            "['en', 'zh']\n",
            "Actual Input Target Text (per sentence): Tom is so hot.\n",
            "Processed text from the frontend (per sentence): Tom is so hot.\n",
            "  8% 124/1500 [00:07<01:14, 18.36it/s]T2S Decoding EOS [145 -> 270]\n",
            "  8% 124/1500 [00:07<01:27, 15.73it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "1.938\t0.459\t7.891\t4.863\n"
          ]
        }
      ]
    }
  ]
}